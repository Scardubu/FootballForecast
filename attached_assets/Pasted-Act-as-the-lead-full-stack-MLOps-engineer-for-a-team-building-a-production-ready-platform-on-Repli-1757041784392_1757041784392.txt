Act as the lead full-stack + MLOps engineer for a team building a production-ready platform on Replit. Your goal is to design, implement, and deliver a complete, operational system that generates calibrated, explainable betting insights (e.g., predicted outcomes, probabilities, key factors like xG, form, and injuries) for user-selected upcoming fixtures in the top 6 soccer leagues (Premier League, La Liga, Serie A, Bundesliga, Ligue 1, Primeira Liga). Optimize for Replit’s environment: use Python as the primary language for compatibility, leverage Replit’s built-in features like secrets for API keys/proxies (if free ones are used), .replit configuration for run commands, and lightweight persistence options (e.g., SQLite for local database fallback instead of full PostgreSQL to avoid external hosting needs; use Replit’s file system for CSVs). Handle Replit’s resource constraints by minimizing CPU-intensive tasks (e.g., batch scraping, efficient ML models), ensuring the app runs in a web-hosted mode (e.g., via Flask/Streamlit for UI, exposed on Replit’s domain), and implementing graceful degradation for network limits (e.g., shorter timeouts, smaller proxy pools).
Constraints and Data Sourcing
	•	No paid APIs or services allowed; rely exclusively on ethical, resilient scraping and crawling pipelines using free, open-source tools like Scrapy, BeautifulSoup, and Playwright (install via pip in Replit, but pre-install in requirements.txt).
	•	Source data from trusted public websites: FBref (advanced stats, xG), WhoScored (player/team ratings), SofaScore (live match data, momentum), Transfermarkt (squads, injuries, market values), Understat (xG data), and official league/club websites.
	•	Implement resilience tailored to Replit: Use in-memory caching (e.g., functools.lru_cache) or local file caching, automatic retries (with exponential backoff via tenacity library), and fallback storage. Persist scraped data as structured, season-based CSVs in Replit’s file system or in a SQLite database for offline use. If live scraping fails due to unavailability, rate limits, errors, or Replit’s IP restrictions, automatically load from these archives.
	•	Incorporate stealth practices to avoid detection and throttling, considering Replit’s shared IPs: Rotate user agents (via fake-useragent), use free proxy pools (e.g., from public lists, stored in Replit secrets), enable headless browser stealth modes (e.g., Playwright with stealth plugins like playwright-stealth), and randomize request intervals (e.g., 10-30 seconds to respect Replit’s rate limits and site policies).
Development Process
Start with a thorough codebase audit in Replit:
	•	Create an architecture map (e.g., high-level diagram as a Markdown file or ASCII art in docs).
	•	Generate a dependency graph (e.g., using pipdeptree, output to a file).
	•	Prioritize issues (e.g., Replit-specific: memory leaks, long-running processes, dependency conflicts).
Then, proceed incrementally with the following integration steps, committing to Replit’s Git integration for version control:
	1	Build robust data ingestion pipelines for scraping match stats, odds, injuries, lineups, and form data, runnable as background tasks (e.g., via schedule library).
	2	Perform feature engineering (e.g., derive features from xG, recent form streaks, injury impacts using pandas).
	3	Develop time-series validated and calibrated ML models (e.g., using scikit-learn or XGBoost for predictions, with calibration via Platt scaling; evaluate with Brier score and log loss; train on historical data scraped once and stored).
	4	Create an inference API (e.g., FastAPI or Flask) for serving insights, hosted on Replit’s web server.
	5	Design a responsive, intuitive UI (e.g., using Streamlit for simplicity in Replit, with components for fixture selection, insight charts via Plotly, and explanations).
Meet these performance and quality targets, adjusted for Replit:
	•	Latency: P95 ≤ 400 ms for cached responses, ≤ 3s for fresh scrapes (accounting for Replit’s variable network).
	•	Data freshness: ≤ 30 minutes for key updates (e.g., injuries, lineups; use scheduled jobs).
	•	Accuracy: ≥ 5% lift over baseline models (measured by Brier score or log loss reduction).
	•	Reproducibility: Implement simple CI/CD via Replit’s deploy button or GitHub sync, unit/integration tests (≥80% coverage with pytest), containerization optional (Replit handles runtime), monitoring (e.g., basic logging to console/files), and secure secret management (use Replit’s Secrets panel for proxies/user agents, no hardcoding).
Deliverables
Provide the following in your response, structured as incremental steps (e.g., small PR-like updates suitable for Replit’s fork/share model):
	•	Scraping strategy document: Detail proxy rotation (e.g., cycle through 5-10 free proxies), fallback design (SQLite queries on failure), and ethical considerations (e.g., respect robots.txt, limit to 1 request/min per site, add disclaimers on fair use).
	•	Integration roadmap: Phased timeline with milestones (e.g., Week 1: Scrapers + DB; Week 2: ML + API) and dependencies, as a Markdown file.
	•	Architectural Decision Records (ADRs): For key choices like using SQLite over PostgreSQL (for Replit portability), Streamlit for UI (quick prototyping), or model selection, stored in an /docs/adr/ folder.
	•	Diagrams: ASCII or simple text-based representations (e.g., for architecture, data flow; embed in README.md).
	•	Code increments: Simulate small PRs with file changes (e.g., snippets for scrapers in src/scrapers.py, models in src/models.py, API in app.py), tests (in tests/test_scrapers.py), and verification commands (e.g., ‘python -m pytest’, ‘streamlit run app.py’).
	•	Runbooks: Operational guides for setup (e.g., ‘pip install -r requirements.txt; sqlite3 data.db < schema.sql’), deployment (fork Replit, hit Run), troubleshooting (e.g., proxy failures: refresh secrets), and maintenance (e.g., manual scrape refresh via console command).
	•	Demo script: A step-by-step script or commands to run the full platform in Replit (e.g., 1. Fork repo; 2. Add secrets; 3. Run ‘python init_db.py’; 4. ‘streamlit run app.py’; 5. Select fixture in UI, view insights), demonstrating end-to-end functionality with sample data.
Keep all detailed reasoning internal; in your output, expose only high-level rationale, commands, file paths/changes, and verification steps per increment. Ensure the final delivery is a fully simulated, operational platform description with exceptional user experience (e.g., clean Streamlit UI with dropdowns, interactive charts, explainable insights via SHAP), ready for real-world use if implemented on Replit.
